---
title: "spacy_regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
language_list=c("English (American)")    #"English (American)"
word_class = c("all", "nouns","adjectives","verbs","function_words","other")

```


### Load saved CHILDES .csv corpus for a language (here French and Italian). 

Rbind different corpora.

```{r libraries, results='hide'}
library(tidyverse)
library(lme4)
library(ggeffects)
library(wordbankr)
library(psych)
library(reshape2)
library(ggpubr)
library(rstatix)
library(data.table)
library(gridExtra)
library(here)

source("get_wordbank.R")
source("measure_frequency.R")

```

```{r load_data, message=FALSE}

clean_childes <- function(corpus_) {
  annot <- c("xxx", "yyy", "www", "-", "'") 
  annotUtt <- filter(corpus_, lemma %in% annot) 
  annotUttID<- unique(annotUtt$utterance_id) 
  corpus_ <- filter(corpus_, !(utterance_id  %in% annotUttID))  #remove utterances with annotations - incomplete info
  corpus_<-corpus_ %>% filter (speaker_code != "CHI") #remove target child utterances
  corpus_<-corpus_ %>% filter (pos != "PUNCT")  #remove punctuation 
  corpus_ %>% mutate(lemma = tolower(lemma))
}

load_data <- function(language_list) {
  for (lang in language_list){
    if (lang == "English (American)")     { 
      english=read_csv(here("data/Providence_spacy.csv"))
      english=clean_childes(english)
      english <- english %>% 
            mutate(language = "English")
    }
  }
  return(list(english = data.table(english)))
}

```
### Clean CHILDES corpus. 

Clean utterances by removing puctuation, incomplete sentences and target-child speech. 

```{r example, message=FALSE}

corpus<-load_data(language_list) 
corpus <- lapply(corpus, clean_childes)

lapply(corpus, function(x) {
  summary(x)
})


```



```{r example frequency}

frequencies <- lapply(corpus, frequency_model) %>%
  bind_rows()

# TEST1 frequency metric:
frequencies %>% 
  group_by(target_child_id) %>% 
  summarize (sum(rawFrequency)) #Test frequence: should be 1 for each child

# TEST2 frequency metric:   
frequencies %>% 
  arrange(desc(FrequencyLog))#: maximum values

```

```{r get_aoas}
aoas <- lapply(language_list, load_wordbank) %>%
  bind_rows()
``` 

```{r merge_frequency_aoas}
d <- aoas %>%
  group_by(language, lemma, lexical_class) %>%
  summarise(aoa = aoa[1]) %>%
  filter(!is.na(aoa)) %>%
  mutate(language = ifelse(language == "French (French)", "French", language)) %>%
  mutate(language = ifelse(language == "English (American)", "English", language)) %>%
  left_join(frequencies %>% 
              group_by(language, lemma) %>%
              summarise(log_freq = FrequencyLogMean[1], 
                        intercept_freq = interceptmodel[1]))

```
##########################################
### Plot frequency

Plot frequency and aoa using log frequency and model intercept frequency:
```{r plot}

plot_frequency1<-function(db){
  ggplot(db, 
         aes(log_freq, aoa, label=lemma)) + 
    geom_point()  +
    geom_text(aes(label=lemma),hjust=0, vjust=0) +xlim(0,0.6) + facet_wrap(~lexical_class, nrow=2)
}

plot_frequency2<-function(db){
  ggplot(db, 
         aes(intercept_freq, aoa, label=lemma)) + 
    geom_point()  +
    geom_text(aes(label=lemma),hjust=0, vjust=0) + facet_wrap(~lexical_class, nrow=2)
}

plot_frequency3<-function(db){
  ggplot(db, 
         aes(x = intercept_freq, y = log_freq, label = lemma)) + 
    geom_point() + 
    geom_smooth(method = "lm") +   facet_wrap(~lexical_class, nrow=2)
}

plot_frequency1(filter(d, language == "English"))

ggplot(d, aes(x = log_freq, y = aoa, col = lexical_class)) + 
  geom_point(alpha = .1) + 
  geom_smooth(method = "lm") + 
  facet_grid(language ~ lexical_class, scales = "free_x") + 
  langcog::theme_mikabr() + 
  langcog::scale_color_solarized() + 
  theme(legend.position = "bottom") + 
  xlab("Frequency (log)") + 
  ylab("Age of Acquisition (months)")

```

### Reliability_frequency: half-split and Spearman-Brown

```{r reliability_frequency}

# add lemmas of the first half not existing at the second half, with 1 
same_size_df <- function(df1, df2) { 
  firstlistlemma<-(df1$name)
  secondlistlemma<-(df2$name)
  diff1<-setdiff(firstlistlemma,secondlistlemma) 
  df<-as.data.frame(diff1)
  df[,2] <- NA
  df[,3] <- 1
  colnames(df)<- c("lemma","pos","CountLemma")
  df$name = df$lemma
  secondhalf<- rbind(df2, df)
  return(secondhalf)
}


split_half_cor <-function(dataAoa, corpus){
  n<-nrow(corpus) #corpus size in word tokens
  
  wblemmas<-unique(dataAoa$lemma) #unique wordbank lemmas
  
  ind <- sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.5, 0.5)) #randomly split word tokens
  firsthalf <- corpus[ind, ] #split in two
  secondhalf <- corpus[!ind, ]
  
  firsthalf <- firsthalf %>%  
    group_by(lemma, pos) %>% 
    summarize(CountLemma=n()) #group by lemma and pos and count raw frequency
  
  secondhalf <- secondhalf %>%  
    group_by(lemma, pos) %>% 
    summarize(CountLemma=n()) 
  
  secondhalf <- secondhalf[secondhalf$lemma %in% wblemmas, ] #keep only lemmas existing in wordbank
  firsthalf <- firsthalf[firsthalf$lemma %in% wblemmas, ]
  
  firsthalf$name <- paste(firsthalf$lemma, "-", firsthalf$pos) #merge lemma and pos to a new name, just in case
  secondhalf$name <- paste(secondhalf$lemma, "-", secondhalf$pos)
  
  firsthalf<-firsthalf[order(firsthalf$name),] #order vector alphabetically
  secondhalf<-secondhalf[order(secondhalf$name),]
  
  secondhalf<- same_size_df(firsthalf, secondhalf)
  firsthalf<-same_size_df(secondhalf, firsthalf)
  
  firsthalf<-firsthalf[order(firsthalf$name),] #order again
  secondhalf<-secondhalf[order(secondhalf$name),]
  
  r<-cor(firsthalf$CountLemma, secondhalf$CountLemma, method="kendall") #measure r
  return(r)
}

sbformula <- function(r){  #adjust with spearman-brown formula
  r1<-(2*r)/(1+r)
  return(r1)
}

``` 


```{r reliability_alpha}

cronbach_alpha <-function(dataAoa, corpus_frequency_){
  corpus_frequency_reliability <- corpus_frequency_ %>% 
    ungroup() %>% 
    select(lemma, rawFrequency, target_child_id)
  
  wblemmas<-unique(dataAoa$lemma) #unique wordbank lemmas
  corpus_frequency_reliability  <- corpus_frequency_reliability [corpus_frequency_reliability $lemma %in% wblemmas, ] #keep only lemmas with corresponding items in wordbank
  
  lemma_<-corpus_frequency_reliability$lemma  #restructure dataframe
  target_child_id_<-corpus_frequency_reliability$target_child_id
  freq_<-corpus_frequency_reliability$rawFrequency
  
  df<-data.frame(lemma_, target_child_id_, freq_)
  corpus_frequency_reliability_<-tidyr::spread(df, target_child_id_, freq_)
  
  child_ids_<- unique(as.character(colnames(corpus_frequency_reliability_)[3:ncol(corpus_frequency_reliability_)]))
  
  child<-select(corpus_frequency_reliability_, child_ids_ )
  a<-alpha(child) 
#  return(a$raw_)
 return(a$total[1,1]) 
}

```

```{r apply_reliability_frequency}

reliabilities <- expand_grid(language = c("english"), 
                            word_class = c("all", "nouns","adjectives","verbs",
                                           "function_words","other")) %>% 
  rowwise %>%
  mutate(split_half_tau = ifelse(word_class == "all", 
                             split_half_cor(aoas, corpus[[language]]),
                             split_half_cor(filter(aoas, 
                                                   lexical_class == word_class),
                                            corpus[[language]])),
         split_half_tau_sb = sbformula(split_half_tau),
      cronbach_alpha = ifelse(word_class == "all", 
                  cronbach_alpha(aoas, 
                                 filter(frequencies, 
                                        language == str_to_title(language))),
                  cronbach_alpha(filter(aoas, 
                                        lexical_class == word_class),
                                 filter(frequencies, 
                                        language == str_to_title(language)))))


reliabilities %>%
  knitr::kable(digits = 2)

```  

### Reliability_AoA:

```{r reliability_aoa}  

split_half_cor_aoa <-function(lang_, clas_){
  
  i<-get_item_data(language = lang_,form="WS")
  i<-i %>% filter(type=="word") #get item data and filter by lexical class
  if (clas_ != ""){
    i<-i %>% filter(lexical_class==clas_)  
    }
  ids<-unique(i$item_id)
  ids<-lapply(X = ids, FUN = function(t) gsub(pattern = "item_", replacement = "", x = t, fixed = TRUE))
  
  items<-get_instrument_data(language = lang_,form="WS", administrations = TRUE) #get instrument data and filter by item
  items<-items %>% filter(num_item_id %in% ids)
  
  admin<-as.data.frame(unique(items$data_id))
  n<-nrow(admin) #corpus size in word tokens
  ind <- sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.5, 0.5)) #randomly split administrations
  
  adminfirstnum <- admin[ind, ]
  adminsecondnum <- admin[!ind, ] #create two groups of administrations
  
  adminfirst<-items %>% filter(data_id %in% adminfirstnum) #filter items in administrations
  adminsecond<-items %>% filter(data_id %in% adminsecondnum)
  
  aoafirst<- fit_aoa(adminfirst, method = "glmrob", proportion = 0.5) # get aoa for each group
  aoasecond<- fit_aoa(adminsecond, method = "glmrob", proportion = 0.5) # 
  
  r<-cor(aoafirst$aoa, aoasecond$aoa, use="complete.obs", method="kendall") #measure r
  return(r)
}

```

```{r apply_reliability_aoa}

reliability_aoa <- expand_grid(language = c("English (American)"), 
                            word_class = c("all", "nouns","adjectives","verbs",
                                           "function_words","other")) %>% 
  rowwise %>%
  mutate(split_half_aoa = ifelse(word_class == "all", 
                             split_half_cor_aoa(language, ""),
                             split_half_cor_aoa(language, word_class)),
         split_half_aoa_sb = sbformula(split_half_aoa))

reliability_aoa %>%
  knitr::kable(digits = 2)

```

### Regression

```{r regression}

regression_option1<-function(db){
  db <- db[!is.na(db$log_freq),]
  option1<-lm(aoa~ log_freq, data=db) 
  return(summary(option1)$adj.r.squared)
}



r2 <- expand_grid(lang = c("English"), 
                            class = c("all", "nouns","adjectives","verbs",
                                           "function_words","other")) %>% 
  rowwise %>%
   mutate(r2 = ifelse(class == "all", 
                             regression_option1(filter(d, 
                                                   language == lang)),
                             regression_option1(filter(d, 
                                                   language == lang, lexical_class == class)))) %>%
  rename( language = lang,lexical_class = class) %>%
  left_join(d)

```






```{r final plot}



```
