---
title: "spacy_regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
language_list=c("italian", "english", "french")  


####TODO: add to aoa-pipeline: clean childes, intercept frequency
```


### Load saved CHILDES .csv corpus for a language (here French and Italian). 

Rbind different corpora.

```{r libraries, results='hide'}
library(tidyverse)
library(lme4)
library(ggeffects)
library(wordbankr)
library(psych)
library(reshape2)
library(ggpubr)
library(rstatix)
library(data.table)
library(gridExtra)
library(here)
library(langcog)
library(modelr)
#theme_set(theme_mikabr())
#font <- theme_mikabr()$text$family

source("get_wordbank.R")

```

### Load data and clean CHILDES utterances by removing puctuation, incomplete sentences and target-child speech. 

```{r load_data, message=FALSE}

#add this function to aoa
clean_childes <- function(corpus_) {
  annot <- c("xxx", "yyy", "www", "-", "'") 
  annotUtt <- filter(corpus_, lemma %in% annot) 
  annotUttID<- unique(annotUtt$utterance_id) 
  corpus_ <- filter(corpus_, !(utterance_id  %in% annotUttID))  #remove utterances with annotations - incomplete info
  corpus_<-corpus_ %>% filter (speaker_code != "CHI") #remove target child utterances
  corpus_<-corpus_ %>% filter (pos != "PUNCT")  #remove punctuation 
  corpus_ %>% mutate(lemma = tolower(lemma))
}

load_data <- function(language_list) {
  for (lang in language_list){
    if (lang == "english")     { 
      english=read_csv(here("data/Providence_spacy.csv"))
      english=clean_childes(english)
      english <- english %>% 
            mutate(language = "english")
    }
    if (lang == "italian")     { 
      italian=read_csv(here("data/italian_1403.csv"))
      italian=clean_childes(italian)
      italian <- italian %>% 
            mutate(language = "italian")
    }
    if (lang == "french")     { 
      french=read_csv(here("data/french_1403.csv"))
      french=clean_childes(french)
      french <- french %>% 
            mutate(language = "french")
    }
  }
  return(list(english = data.table(english), italian = data.table(italian), french = data.table(french)))
  #return(list(english = data.table(english), italian = data.table(italian)))

  }

```

```{r example, message=FALSE}

corpus<-load_data(language_list) 
#corpus <- lapply(corpus, clean_childes)

lapply(corpus, function(x) {
  summary(x)
})


```

### Measure frequency

```{r example frequency}
source("measure_frequency.R") # add interecept function to aoa-pipeline

frequencies <- lapply(corpus, frequency_model) %>%
  bind_rows()

# TEST1 frequency metric:
#frequencies %>% 
 # group_by(target_child_id) %>% 
  #summarize (sum(rawFrequency)) #Test frequence: should be 1 for each child

# TEST2 frequency metric:   
#frequencies %>% 
 # arrange(desc(FrequencyLog))#: maximum values

```

### Measure aoas

```{r get_aoas}
language_list_=c("Italian", "English (American)", "French (French)")  

aoas <- lapply(language_list_, load_wordbank) %>%
  bind_rows()
``` 

### Merge all

```{r merge_frequency_aoas}
d <- aoas %>%
  group_by(language, lemma, lexical_class) %>%
  summarise(aoa = aoa[1]) %>%
  filter(!is.na(aoa)) %>%
  mutate(language = ifelse(language == "French (French)", "french", language)) %>%
  mutate(language = ifelse(language == "English (American)", "english", language)) %>%
  mutate(language = ifelse(language == "Italian", "italian", language)) %>%
  left_join(frequencies %>% 
              group_by(language, lemma) %>%
              summarise(log_freq = FrequencyLogMean[1], 
                        intercept_freq = interceptmodel[1])   )

```

### Plot frequency and aoa using log frequency and model intercept frequency
```{r plot}

d<-d %>% 
  filter(!is.na(log_freq)) 


plot_frequency1<-function(db, language){
  ggplot(db, 
         aes(log_freq, aoa, label=lemma)) + 
    geom_point()  +
    geom_smooth() + 
    geom_point(alpha=.1)+
    ggrepel::geom_label_repel()+
    geom_text(aes(label=lemma),hjust=0, vjust=0)  +
    xlim(0,0.6) + 
    facet_wrap(~lexical_class, nrow=2) +
    ggtitle(paste(language)) 
    
}

plot_frequency2<-function(db){
  ggplot(db, 
         aes(intercept_freq, aoa, label=lemma)) + 
    geom_point()  +
    geom_text(aes(label=lemma),hjust=0, vjust=0) + facet_wrap(~lexical_class, nrow=2)
}

plot_frequency3<-function(db){
  ggplot(db, 
         aes(x = intercept_freq, y = log_freq, label = lemma)) + 
    geom_point() + 
    geom_smooth(method = "lm") +   facet_wrap(~lexical_class, nrow=2)
}

```
```{r freq_plot_1}
plot_frequency1(filter(d, language == "english"), "english")
plot_frequency1(filter(d, language == "italian"), "italian")
plot_frequency1(filter(d, language == "french"), "french")

```


```{r final_plot_1}

ggplot(d, aes(x = log_freq, y = aoa, col = lexical_class)) + 
  geom_point(alpha = .1) + 
  geom_smooth(method = "lm") + 
  facet_grid(rows = vars(language)) +
 # facet_grid(language ~ lexical_class, scales = "free_x") + 
 # langcog::theme_mikabr() + 
 # langcog::scale_color_solarized() + 
  theme(legend.position = "bottom") + 
  xlab("Frequency (log)") + 
  ylab("Age of Acquisition (months)")


```
```

### Reliability_frequency: half-split and Spearman-Brown

```{r reliability_frequency}

# add lemmas of the first half not existing at the second half, with 1 
same_size_df <- function(df1, df2) { 
  firstlistlemma<-(df1$name)
  secondlistlemma<-(df2$name)
  diff1<-setdiff(firstlistlemma,secondlistlemma) 
  df<-as.data.frame(diff1)
  df[,2] <- NA
  df[,3] <- 1
  colnames(df)<- c("lemma","pos","CountLemma")
  df$name = df$lemma
  secondhalf<- rbind(df2, df)
  return(secondhalf)
}


split_half_cor <-function(dataAoa, corpus){
  n<-nrow(corpus) #corpus size in word tokens
  
  wblemmas<-unique(dataAoa$lemma) #unique wordbank lemmas
  
  ind <- sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.5, 0.5)) #randomly split word tokens
  firsthalf <- corpus[ind, ] #split in two
  secondhalf <- corpus[!ind, ]
  
  firsthalf <- firsthalf %>%  
    group_by(lemma, pos) %>% 
    summarize(CountLemma=n()) #group by lemma and pos and count raw frequency
  
  secondhalf <- secondhalf %>%  
    group_by(lemma, pos) %>% 
    summarize(CountLemma=n()) 
  
  secondhalf <- secondhalf[secondhalf$lemma %in% wblemmas, ] #keep only lemmas existing in wordbank
  firsthalf <- firsthalf[firsthalf$lemma %in% wblemmas, ]
  
  firsthalf$name <- paste(firsthalf$lemma, "-", firsthalf$pos) #merge lemma and pos to a new name, just in case
  secondhalf$name <- paste(secondhalf$lemma, "-", secondhalf$pos)
  
  firsthalf<-firsthalf[order(firsthalf$name),] #order vector alphabetically
  secondhalf<-secondhalf[order(secondhalf$name),]
  
  secondhalf<- same_size_df(firsthalf, secondhalf)
  firsthalf<-same_size_df(secondhalf, firsthalf)
  
  firsthalf<-firsthalf[order(firsthalf$name),] #order again
  secondhalf<-secondhalf[order(secondhalf$name),]
  
  r<-cor(firsthalf$CountLemma, secondhalf$CountLemma, method="kendall") #measure r
  return(r)
}

sbformula <- function(r){  #adjust with spearman-brown formula
  r1<-(2*r)/(1+r)
  return(r1)
}

``` 

### Reliability_frequency: cronbach alpha

```{r reliability_alpha}

cronbach_alpha <-function(dataAoa, corpus_frequency_){
  corpus_frequency_reliability <- corpus_frequency_ %>% 
    ungroup() %>% 
    select(lemma, rawFrequency, target_child_id)
  
  wblemmas<-unique(dataAoa$lemma) #unique wordbank lemmas
  corpus_frequency_reliability  <- corpus_frequency_reliability [corpus_frequency_reliability $lemma %in% wblemmas, ] #keep only lemmas with corresponding items in wordbank
  
  lemma_<-corpus_frequency_reliability$lemma  #restructure dataframe
  target_child_id_<-corpus_frequency_reliability$target_child_id
  freq_<-corpus_frequency_reliability$rawFrequency
  
  df<-data.frame(lemma_, target_child_id_, freq_)
  corpus_frequency_reliability_<-tidyr::spread(df, target_child_id_, freq_)
  
  child_ids_<- unique(as.character(colnames(corpus_frequency_reliability_)[3:ncol(corpus_frequency_reliability_)]))
  
  child<-select(corpus_frequency_reliability_, child_ids_ )
  a<-alpha(child) 
#  return(a$raw_)
 return(a$total[1,1]) 
}

```

### Measure reliabilities

```{r apply_reliability_frequency}

reliabilities <- expand_grid(language = c( "english", "italian", "french"), 
                            word_class = c("all", "nouns","adjectives","verbs",
                                           "function_words","other")) %>% 
  rowwise %>%
  mutate(split_half_tau = ifelse(word_class == "all", 
                             split_half_cor(aoas, corpus[[language]]),
                             split_half_cor(filter(aoas, 
                                                   lexical_class == word_class),
                                            corpus[[language]])),
         split_half_tau_sb = sbformula(split_half_tau) )#,
    #  cronbach_alpha = ifelse(word_class == "all", 
     #             cronbach_alpha(aoas, 
      #                           filter(frequencies, 
       #                                 language == language)),
        #          cronbach_alpha(filter(aoas, 
         #                               lexical_class == word_class),
          #                       filter(frequencies, 
           #                             language == language))))

reliabilities %>%
  knitr::kable(digits = 2)
reliabilities <- reliabilities %>%
    mutate(language = sub("english", "English (American)", language)) %>%
    mutate(language = sub("italian", "Italian", language)) %>%
    mutate(language = sub("french", "French (French)", language))

```  

### Reliability_AoA

```{r reliability_aoa}  

split_half_cor_aoa <-function(lang_, clas_){
  
  i<-get_item_data(language = lang_,form="WS")
  i<-i %>% filter(type=="word") #get item data and filter by lexical class
  if (clas_ != ""){
    i<-i %>% filter(lexical_class==clas_)  
    }
  if (lang_ == "French (French)") {
   i <- filter(i, item_id !="item_514")
   i <- filter(i, item_id !="item_628")
   i <- filter(i, item_id !="item_601")
   i <- filter(i, item_id !="item_627")
   i <- filter(i, item_id !="item_452")
   i <- filter(i, item_id !="item_599")
    }  
  ids<-unique(i$item_id)
  ids<-lapply(X = ids, FUN = function(t) gsub(pattern = "item_", replacement = "", x = t, fixed = TRUE))
  
  items<-get_instrument_data(language = lang_,form="WS", administrations = TRUE) #get instrument data and filter by item
  items<-items %>% filter(num_item_id %in% ids)
  
  admin<-as.data.frame(unique(items$data_id))
  n<-nrow(admin) #corpus size in word tokens
  ind <- sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.5, 0.5)) #randomly split administrations
  
  adminfirstnum <- admin[ind, ]
  adminsecondnum <- admin[!ind, ] #create two groups of administrations
  
  adminfirst<-items %>% filter(data_id %in% adminfirstnum) #filter items in administrations
  adminsecond<-items %>% filter(data_id %in% adminsecondnum)
  
  aoafirst<- fit_aoa(adminfirst, method = "glmrob", proportion = 0.5) # get aoa for each group
  aoasecond<- fit_aoa(adminsecond, method = "glmrob", proportion = 0.5) # 
  
  r<-cor(aoafirst$aoa, aoasecond$aoa, use="complete.obs", method="kendall") #measure r
  return(r)
}

```

```{r apply_reliability_aoa}

reliabilities_aoa <- expand_grid(language = c("French (French)","English (American)", "Italian"),
                            word_class = c("all", "nouns","adjectives","verbs",
                                           "function_words","other")) %>% 
  rowwise %>%
  mutate(split_half_aoa = ifelse(word_class == "all", 
                             split_half_cor_aoa(language, ""),
                             split_half_cor_aoa(language, word_class)),
         split_half_aoa_sb = sbformula(split_half_aoa))

reliabilities_aoa %>%
  knitr::kable(digits = 2)

```

### Regression

```{r main_regression}

regression_option1<-function(db){
  db <- db[!is.na(db$log_freq),]
  option1<-lm(aoa~ log_freq, data=db) 
  return(summary(option1)$adj.r.squared)
}

r2 <- expand_grid(lang = c("english", "italian", "french"), 
                            class = c("all", "nouns","adjectives","verbs",
                                           "function_words","other")) %>% 
  rowwise %>%
   mutate(r2 = ifelse(class == "all", 
                             regression_option1(filter(d, 
                                                   language == lang)),
                             regression_option1(filter(d, 
                                                   language == lang, lexical_class == class)))) %>%
  rename( language = lang,lexical_class = class) %>%
  left_join(d)

```

```{r cross_validate_regression}

#divide into training and test set
### (1)Use this formula after freezing all coefficients: 1 - (sum of squared errors) / (sum of squares total). The denominator is (ùëõ‚àí1)√ó the observed variance of ùëå in the holdout sample.
### (2)1- sum squared differences between the predicted and observed value / sum of squared differences between the observed and overall mean value
### (3)Calculate mean square error and variance of each group 

xvalr2 <- function(d){
n<-nrow(d) #df size
ind <- sample(c(TRUE, FALSE), n, replace=TRUE, prob=c(0.9, 0.1)) #randomly split lines
train <- d[ind, ] 
test <- d[!ind, ]
model <- lm(aoa~ log_freq, data=train) 
predictions <- predict(model, test)
#crossvalr2 <- 1-(sum((test$aoa - predictions)^2)/(n-1)*var(test$aoa))  (1)
#crossvalr2 <- 1-(sum((predictions - test$aoa)^2)/sum((test$aoa - mean(test$aoa))^2) ) (2)
#crossvalr2 <- 1-(sum((test$aoa - predictions)^2)/var(test$aoa))   (3) 
crossvalr2 <- rsquare(model, test)
return(crossvalr2)
}

crossvalr2 <- expand_grid(lang = c("english", "italian", "french"), 
                            class = c("all", "nouns","adjectives","verbs",
                                           "function_words","other")) %>% 
  rowwise %>%
   mutate(crossvalr2 = ifelse(class == "all", 
                             xvalr2(filter(d, 
                                                   language == lang)),
                             xvalr2(filter(d, 
                                                   language == lang, lexical_class == class)))) %>%
  rename( language = lang,lexical_class = class) 

crossvalr2 %>%
  knitr::kable(digits = 2)
```

```{r final_data_r2}

all_r2 <- r2 %>%
  left_join(crossvalr2) %>%
  select(language, lexical_class, r2, crossvalr2) %>%
  distinct() %>%
  rename(word_class = lexical_class) %>%
  mutate(language = sub("english", "English (American)", language)) %>%
  mutate(language = sub("italian", "Italian", language)) %>%
  mutate(language = sub("french", "French (French)", language))

all_r2 %>%
  knitr::kable(digits = 2)
```

```{r final_data_reliab}  
all_reliabilities <- reliabilities %>%
  left_join(reliabilities_aoa) %>%
    mutate(threshold_half = split_half_tau_sb * split_half_aoa_sb) 
    # %>%
    # mutate(threshold_alpha = cronbach_alpha * split_half_aoa_sb) 

all_reliabilities %>%
  knitr::kable(digits = 2)
```

```{r final_merged_data} 
dr<- all_reliabilities %>% 
  left_join(unique(all_r2)) 

dr %>%
  knitr::kable(digits = 2)
```

```{r final_plot_2}

ggplot(dr, aes(x = word_class, y=r2, fill=word_class)) + 
  geom_bar(stat="identity") + 
  facet_grid(rows = vars(language)) +
  geom_errorbar(data  = dr, aes(y=threshold_half, ymax=threshold_half, ymin=threshold_half, col=word_class)) + 
  theme(legend.position = "bottom") + 
  xlab("Lexical class") + 
  ylab("R2") + 
  theme(legend.title = element_blank()) 


```


