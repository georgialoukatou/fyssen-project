---
title: "spacy_regression"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
language_="French (French)"    #"English (American)" #Italian
```

## R Markdown

Load databases

```{r clean data, results='hide', message=FALSE}
library(tidyverse)
library(lme4)
library(ggeffects)
library(wordbankr)
library(psych)

#for (language_ in c("Italian", "French", "NAEnglish")){

if (language_ == "Italian") {

antelmi<-read_csv("/Users/lscpuser/Documents/fyssen-project/Antelmi_spacy.csv")
calambrone<-read_csv("/Users/lscpuser/Documents/fyssen-project/Calambrone_spacy.csv")
roma<-read_csv("/Users/lscpuser/Documents/fyssen-project/Roma_spacy.csv")
tonelli<-read_csv("/Users/lscpuser/Documents/fyssen-project/Tonelli_spacy.csv")

corpus<-rbind(antelmi, calambrone, roma, tonelli)
corpus<-corpus %>% mutate(language="Italian")
}

if (language_ == "French (French)") {

lyon <-read_csv("/Users/lscpuser/Documents/fyssen-project/Lyon_spacy.csv")
paris <-read_csv("/Users/lscpuser/Documents/fyssen-project/Paris_spacy.csv")
mtln<-read_csv("/Users/lscpuser/Documents/fyssen-project/MTLN_spacy.csv")
yamaguchi<-read_csv("/Users/lscpuser/Documents/fyssen-project/Yamaguchi_spacy.csv")
goadrose<-read_csv("/Users/lscpuser/Documents/fyssen-project/Goadrose_spacy.csv")

corpus<-rbind(lyon, paris, mtln, yamaguchi, goadrose)
corpus<-corpus %>% mutate(language="French")
}

if (language_ == "English (American)") {
  
corpus<-read.csv("/Users/lscpuser/Documents/fyssen-project/Providence_spacy.csv")
corpus <- corpus %>% mutate(language="NAEnglish")
  }
#}

```


```{r clean corpora}
annot <- c("xxx", "yyy", "www", "-", "'") 

annotUtt <- filter(corpus, lemma %in% annot) 
annotUttID<- unique(annotUtt$utterance_id) 
corpus <- filter(corpus, !(utterance_id  %in% annotUttID))  #remove utterances with annotations - incomplete info

corpus<-corpus %>% filter (speaker_code != "CHI") #remove target child utterances
corpus<-corpus %>% filter (pos != "PUNCT")  #remove punctuation 

corpus$lemma<-tolower(corpus$lemma) # tolower
```



```{r raw frequency}
#Count each lemma and sum of lemmas for each target child
corpus_frequency<- corpus %>%  group_by(lemma, target_child_id, language) %>% summarize(CountLemma=n()) #count each lemma for each child
CountAllLemmas<-corpus_frequency %>%  group_by(target_child_id, language) %>% summarize(CountAllLemmasChild=sum(CountLemma)) #count all lemma tokens for child 
corpus_frequency<-corpus_frequency %>% left_join(CountAllLemmas) #join infos

#Measure frequency
corpus_frequency<-corpus_frequency %>% mutate (rawFrequency = CountLemma / CountAllLemmasChild)
corpus_frequency %>% group_by(target_child_id) %>% summarize (sum(rawFrequency)) #Test frequence: should be 1 for each child

```

```{r log frequency}
corpus_frequency<-corpus_frequency %>% mutate (FrequencyLog =log(1+ rawFrequency * 100))
#corpus_frequency %>% arrange(desc(FrequencyLog)) Test: maximum values

#measure mean frequency
corpus_frequency1<- corpus_frequency %>%  group_by(lemma, language) %>% summarize(FrequencyLogMean=mean(FrequencyLog))
corpus_frequency <- corpus_frequency %>% left_join(corpus_frequency1) 
```

```{r modeling frequency intercept}

corpus_frequency$intercept <-NA

#loop lemmas and fit model for intercept 
for (i in unique(corpus_frequency$lemma)) {
corpus_frequency$intercept<-ifelse(corpus_frequency$lemma ==i, coef(lm(FrequencyLog ~ CountAllLemmasChild  + (1|target_child_id), data=corpus_frequency  %>% filter (lemma==i)))["(Intercept)"], corpus_frequency$intercept) }

head(corpus_frequency)
```

Alternative approach:

```{r}
models <- corpus_frequency %>%
  group_by(language, lemma) %>%
  nest() 

models_ <- models %>% mutate (interceptmodel = map(.x=data, .f=~coef(lm(.x$rawFrequency ~(.x$CountAllLemmasChild-.x$CountLemma), data=data))["(Intercept)"], na.rm = T))

models_<-models_ %>% unnest(interceptmodel)

corpus_frequency_<-corpus_frequency %>% left_join(unique(models_)) 

#models_1 <- models %>% mutate (lapply(data, function(df) ~lm(c(CountLemma, CountAllLemmas-CountLemma), data=df)),)
#models_1 <- models %>% mutate (lapply(data, function(df) mean(CountLemma, data=df)))
#%>%
 # mutate(models = lapply(data, function(df)
  #  glmer(c(CountLemma,CountAllLemmasChild-CountLemma) ~ 1 + (1|target_child_id), data = df)))
```



```{r wordbank}

WB_tokens<- get_item_data(language = language_, form = "WS")
WB_tokens<-WB_tokens %>% filter (type == "word")  #remove grammar

data <- get_instrument_data(language ="French (French)", form = "WS", items = WB_tokens$item_id, administrations = TRUE, iteminfo=TRUE)
names(data)[names(data) == "definition"] <- "lemma"


aoa<- fit_aoa(data, measure = "produces", method = "glmrob", proportion = 0.5) # 145 NAs out of 680
names(aoa)[names(aoa) == "definition"] <- "lemma"

dataAoa <- unique(data) %>% left_join(unique(aoa)) 
```

##########################################

```{r reliability}
library(reshape2)
library(psych)

corpus_frequency_reliability <- corpus_frequency_ %>% ungroup() %>% select(lemma, FrequencyLog, target_child_id)
lemma_<-corpus_frequency_reliability$lemma
target_child_id_<-corpus_frequency_reliability$target_child_id
freq_<-corpus_frequency_reliability$FrequencyLog

df<-data.frame(lemma_, target_child_id_, freq_)
corpus_frequency_reliability_<-tidyr::spread(df, target_child_id_, freq_)

child_ids_<- unique(as.character(colnames(corpus_frequency_reliability_)[3:ncol(corpus_frequency_reliability_)]))

child<-select(corpus_frequency_reliability_, child_ids_ )
alpha(child)

```

```{r merge aoa and childes}

corpus_frequency_short <- corpus_frequency_ %>% ungroup() %>% select(lemma, language, FrequencyLogMean, interceptmodel) #get only important columns 

#number of lemmas before merging: 13307 for French
CHILDES_WB <- merge(x=dataAoa, y=unique(corpus_frequency_short), by="lemma") 
#number of lemmas after merging: 474

CHILDES_WB <- CHILDES_WB[!is.na(CHILDES_WB$value), ] #remove lemmas with no value

CHILDES_WB_short <- CHILDES_WB%>% select(lemma, FrequencyLogMean, interceptmodel, lexical_category, aoa, num_item_id) 
CHILDES_WB_short <- unique(CHILDES_WB_short)
CHILDES_WB_short <- CHILDES_WB_short[!is.na(CHILDES_WB_short$aoa), ] # 565 -> 443 after removing NA aoas


head(CHILDES_WB_short)
```

##########################################

```{r plot}

ggplot(CHILDES_WB_short, aes(FrequencyLogMean, aoa, label=lemma)) + geom_point()  +geom_text(aes(label=lemma),hjust=0, vjust=0) +xlim(0,0.6) + facet_wrap(~lexical_category, nrow=2)


ggplot(CHILDES_WB_short, aes(interceptmodel, aoa, label=lemma)) + geom_point()  +geom_text(aes(label=lemma),hjust=0, vjust=0)  + facet_wrap(~lexical_category, nrow=2)

```


```{r}
ggplot(CHILDES_WB_short, 
       aes(x = interceptmodel, y = FrequencyLogMean, label = lemma)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  facet_wrap(~lexical_category)
```


```

```{r plots}

option1<-lmer(aoa~ FrequencyLogMean + (1|lexical_category) + (1|lemma) , data=CHILDES_WB_short) 
anova(option1)
ggpredict(option1, c("FrequencyLogMean")) %>% plot()


CHILDES_WB$value1 <- as.numeric(as.factor(CHILDES_WB$value))
option2 <- glmer(value1 ~ aoa * FrequencyLogMean + (1|lexical_category), data=CHILDES_WB) 
anova(option2)

#qplot( x = CHILDES_WB$intercept, fill = CHILDES_WB$`value == "produces"`, geom = "histogram", main = "Frequency distribution for WB items at 17 months",  xlab = "Frequency of WB items")
```



